{
<<<<<<< HEAD
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/huangtinglin/test_colab/blob/main/CPSC483_colab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWhUwO_-mL2s"
   },
   "source": [
    "## Graph neural network basics\n",
    "\n",
    "In this Colab, we are going to introduce some basics of graph neural network (GNN) and build a pipeline for node classification tasks by PyTorch Geometric (PyG). See more introduction about [PyG](https://pytorch-geometric.readthedocs.io/en/latest/).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0R0-JXHoqhtD"
   },
   "source": [
    "## Outline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETQPVYpoqsvY"
   },
   "source": [
    "- Basic operation of PyG\n",
    "- Build a GNN by PyG\n",
    "- Training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBRS0TB5mo7o"
   },
   "source": [
    "## Basic operation of PyG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N3aXUWpFlcnj",
    "outputId": "3f1d6895-9a19-49ca-86e3-164f82f70b81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch 1.12.1+cu113\n"
     ]
=======
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/huangtinglin/test_colab/blob/main/CPSC483_colab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWhUwO_-mL2s"
      },
      "source": [
        "## Graph neural network basics\n",
        "\n",
        "In this Colab, we are going to introduce some basics of graph neural network (GNN) and build a pipeline for node classification tasks by PyTorch Geometric (PyG). See more introduction about [PyG](https://pytorch-geometric.readthedocs.io/en/latest/).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R0-JXHoqhtD"
      },
      "source": [
        "## Outline\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETQPVYpoqsvY"
      },
      "source": [
        "- Basic operation of PyG\n",
        "- Build a GNN by PyG\n",
        "- Training and testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBRS0TB5mo7o"
      },
      "source": [
        "## Basic operation of PyG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3aXUWpFlcnj",
        "outputId": "3f1d6895-9a19-49ca-86e3-164f82f70b81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using torch 1.12.1+cu113\n"
          ]
        }
      ],
      "source": [
        "# import the pytorch library into environment and check its version\n",
        "import os\n",
        "import torch\n",
        "print(\"Using torch\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC5DJkYNWGeL"
      },
      "source": [
        "Let's start installing PyG by `pip`. The version of PyG should match the current version of PyTorch. Here we follow the [instruction](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html) of PyG:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NWtNbC9FgZOM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6345ee90-007a-4326-c028-972d0c118824"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 12.6 MB/s \n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_sparse-0.6.15-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 58.9 MB/s \n",
            "\u001b[?25hCollecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl (2.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 21.9 MB/s \n",
            "\u001b[?25hCollecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (709 kB)\n",
            "\u001b[K     |████████████████████████████████| 709 kB 50.8 MB/s \n",
            "\u001b[?25hCollecting torch-geometric\n",
            "  Downloading torch_geometric-2.1.0.post1.tar.gz (467 kB)\n",
            "\u001b[K     |████████████████████████████████| 467 kB 15.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.7.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.1.0.post1-py3-none-any.whl size=689859 sha256=cfe188a0abaf616cabdce85b38c3e5937ae23d1bb42be26fcd5674d49fcb81c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/cb/43/f7f2e472de4d7cff31bceddadc36d634e1e545fbc17961c282\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-spline-conv, torch-sparse, torch-scatter, torch-geometric, torch-cluster\n",
            "Successfully installed torch-cluster-1.6.0 torch-geometric-2.1.0.post1 torch-scatter-2.0.9 torch-sparse-0.6.15 torch-spline-conv-1.2.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.4-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 230 kB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.24.3)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (4.64.0)\n",
            "Collecting outdated>=0.2.0\n",
            "  Downloading outdated-0.2.1-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.0.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.3.5)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.12.1+cu113)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.21.6)\n",
            "Collecting littleutils\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (2.23.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->ogb) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=c559dde34c613b4eaa975981a86e1b813719f2c4ace3512fcce1cfb7c822862e\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/64/cd/32819b511a488e4993f2fab909a95330289c3f4e0f6ef4676d\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.4 outdated-0.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
        "!pip install ogb  # for datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei5_O8ZXtlGb"
      },
      "source": [
        "### Create a Graph\n",
        "\n",
        "A single graph in PyG is described by an instance of `torch_geometric.data` which holds the some important attributes by default, like edge_index. We can easily create a graph of various number of edges and nodes by PyG. Take the following graph as an example:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNeApmAljKyq"
      },
      "source": [
        "![](https://github.com/Graph-and-Geometric-Learning/CPSC483-colab/blob/main/fig/graph_example.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZSndMSIJhvC6"
      },
      "outputs": [],
      "source": [
        "# import torch_geometric.data into environment\n",
        "from torch_geometric.data import Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6Usm6XTk22d"
      },
      "source": [
        "We have 6 edges (undirected graph) and 3 nodes in this graph. So the edge index can be defined as:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fAqsazT4lPAZ"
      },
      "outputs": [],
      "source": [
        "edge_index = torch.tensor([[0, 1, 1, 2, 0, 2],\n",
        "                           [1, 0, 2, 1, 2, 0]], dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fw5y3_Ulx1T"
      },
      "source": [
        "Besides, each node can have a node feature which describes the node's property:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3doHCX65l63N"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([[-1], [0], [1]], dtype=torch.float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGKFW9sRlnFK"
      },
      "source": [
        "Then we can define a `Data` object with edge index and node attribute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6mmud5UqlfM9"
      },
      "outputs": [],
      "source": [
        "data = Data(x=x, edge_index=edge_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Af1qVLlmCNb"
      },
      "source": [
        "`Data` object supports many useful utility functions. For example, we can see the number of the nodes, and whether the graph is a undirected graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ybw_wwuStyHo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba68da22-599c-43f0-cdcf-793cd184b508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of nodes is: 3\n",
            "graph is directed or not: False\n"
          ]
        }
      ],
      "source": [
        "num_nodes = data.num_nodes\n",
        "print(\"number of nodes is:\", num_nodes)\n",
        "\n",
        "is_directed = data.is_directed()\n",
        "print(\"graph is directed or not:\", is_directed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCY73L2LGoS8"
      },
      "source": [
        "### Question 1 (5 points)\n",
        "\n",
        "What is the number of the neighbors of node 0 in the graph?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HMg1NUP9Gnkq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a1d32d1-8087-4922-e16c-b167d79c0959"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node with index 0 has 2 neighbors\n"
          ]
        }
      ],
      "source": [
        "def get_n_neighbors(graph, idx):\n",
        "  # TODO: Implement a function that takes a Data object,\n",
        "  # an index of a node, and returns the number of the neighbors \n",
        "  # of this node (as an integer).\n",
        "\n",
        "  n_neighbors = 0\n",
        "\n",
        "  ############# Your code here ############\n",
        "  ## (~1 line of code)\n",
        "\n",
        "  n_neighbors = (data.edge_index[0] == idx).sum().item()\n",
        "\n",
        "  #########################################\n",
        "\n",
        "  return n_neighbors\n",
        "\n",
        "idx = 0\n",
        "n_neighbors = get_n_neighbors(data, idx)\n",
        "print('Node with index {} has {} neighbors'.format(idx, n_neighbors))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKHOYbMCE9b7"
      },
      "source": [
        "PyG has a number of graph data with various scales. Cora is one of the most famous dataset in graph learning, and we can use it by PyG:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "RfZq2L2uEphL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "845e9e74-ab12-43ea-d180-4b6948720fab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.datasets import Planetoid\n",
        "\n",
        "dataset = Planetoid('/tmp/cora', 'cora')\n",
        "data = dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2h5lpJYpjV0"
      },
      "source": [
        "We can see the number of the nodes and edges in cora:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UnYTDzQFpi40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7ebbf53-6146-4703-a152-111d126ae9bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cora has 2708 nodes\n",
            "cora has 10556 edges\n"
          ]
        }
      ],
      "source": [
        "num_nodes = data.num_nodes\n",
        "print('cora has {} nodes'.format(num_nodes))\n",
        "\n",
        "num_edges = data.num_edges\n",
        "print('cora has {} edges'.format(num_edges))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HebClViZqU1V"
      },
      "source": [
        "### Question 2 (10 points)\n",
        "\n",
        "1. What is the number of the classes in cora dataset? \n",
        "2. Which node in Cora has the most number of neighbors?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "pVuXmtFnqr4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49e214e8-c9f0-49ad-98d4-46bbead6d43f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cora has 7 classes\n",
            "1358 in cora has the most number of neighbors\n"
          ]
        }
      ],
      "source": [
        "def get_num_classes(data):\n",
        "  # TODO: Implement a function that takes a dataset object\n",
        "  # and returns the number of classes for that dataset.\n",
        "\n",
        "  num_classes = 0\n",
        "\n",
        "  ############# Your code here ############\n",
        "  ## (~1 line of code)\n",
        "  \n",
        "  num_classes = data.y.unique().shape[0]\n",
        "\n",
        "  #########################################\n",
        "\n",
        "  return num_classes\n",
        "\n",
        "def get_idx_with_most_neighbors(data):\n",
        "  # TODO: Implement a function that takes a dataset object\n",
        "  # and returns the index of the node which has the most number of neighbors.\n",
        "\n",
        "  idx = -1\n",
        "\n",
        "  ############# Your code here ############\n",
        "  ## (~3 line of code)\n",
        "\n",
        "  n_nodes = data.x.shape[0]\n",
        "  degrees = [get_n_neighbors(data, i) for i in range(n_nodes)]\n",
        "  idx = degrees.index(max(degrees))\n",
        "  \n",
        "  #########################################\n",
        "\n",
        "  return idx\n",
        "\n",
        "num_classes = get_num_classes(data)\n",
        "print(\"cora has {} classes\".format(num_classes))\n",
        "\n",
        "idx = get_idx_with_most_neighbors(data)\n",
        "print(\"{} in cora has the most number of neighbors\".format(idx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiQW1Hs46Kpv"
      },
      "source": [
        "In cora, we split the data into train set, validation set and test set by node mask. All the nodes will participate in the message passing process, but we can only assess the train label during training process. This is what we call [transductive learning](https://en.wikipedia.org/wiki/Transduction_(machine_learning))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ioXYJFV-7UU_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd0a9628-95e0-4ba5-91f2-da873ec7c9aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of nodes in train set, 140\n",
            "number of nodes in valid set, 500\n",
            "number of nodes in test set, 1000\n"
          ]
        }
      ],
      "source": [
        "node_feature = data.x\n",
        "\n",
        "train_node_feature = node_feature[data.train_mask]\n",
        "valid_node_feature = node_feature[data.val_mask]\n",
        "test_node_feature = node_feature[data.test_mask]\n",
        "\n",
        "print(\"number of nodes in train set,\", train_node_feature.shape[0])\n",
        "print(\"number of nodes in valid set,\", valid_node_feature.shape[0])\n",
        "print(\"number of nodes in test set,\", test_node_feature.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhLEhOimr7mY"
      },
      "source": [
        "## Build a GNN by PyG\n",
        "\n",
        "In this section we will use PyG to build a classic graph neural network called GCN([Kipf et al. (2017)](https://arxiv.org/pdf/1609.02907.pdf)). Then we will apply this model to handle node classification task in cora.\n",
        "A GCN is built by stacking multiple graph convolution layers `GCNConv` which passes the messages from neighbors to the center node. Here we can define a `GCNConv` by PyG:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "x-8eQfROu7Hm"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "conv = GCNConv(in_channels=1433, out_channels=200, normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxNlpZ5cvI9s"
      },
      "source": [
        "`in_channels` is the dimension of node's input feature, `out_channels` is the  dimension of the output representation of node, and `normalize` is whether to add self-loops and compute symmetric normalization on the adjacent matrix. \n",
        "The feature's dimension in cora is 1433, so `in_channels` is set as 1433. We can perform a message passing on cora like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "MlwiHnm7wy8S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ddb3c58-b508-4611-c5d3-972f2746bcfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dimension of node_feature: torch.Size([2708, 1433])\n",
            "dimension of node_representation: torch.Size([2708, 200])\n"
          ]
        }
      ],
      "source": [
        "node_feature = data.x\n",
        "edge_index = data.edge_index\n",
        "\n",
        "node_representation = conv(node_feature, edge_index)\n",
        "\n",
        "print(\"dimension of node_feature:\", node_feature.shape)\n",
        "print(\"dimension of node_representation:\", node_representation.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxItCC52xud5"
      },
      "source": [
        "We can see that the inputs of `GCNConv` are node feature and edge index. Then the convolution module will perform a message passing like GCN. \n",
        "Recall the MLP we build in colab0. Here we also use `nn.Module` to define a MLP class containing the basic modules of GCN. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eug5Vv-Py5f_"
      },
      "source": [
        "### Question 3 (10 points)\n",
        "\n",
        "Following the instruction and build a GCN class using the `GCNConv` modules. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "iDO3ucqjtBF6"
      },
      "outputs": [],
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: Define two GCNConv modules and a ReLU function.\n",
        "        # The input size and output size of first GCNConv module should be in_channels and hidden_channels\n",
        "        # The input size and output size of second GCNConv module should be hidden_channels and out_channels\n",
        "\n",
        "        ############# Your code here ############\n",
        "        ## (~3 line of code)\n",
        "\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "        self.act = torch.nn.ReLU()\n",
        "\n",
        "        #########################################\n",
        "\n",
        "    def forward(self, node_feature, edge_index):\n",
        "\n",
        "        output = None\n",
        "\n",
        "        # TODO: Use the modules you define in __init__ to perform message passing.\n",
        "        # ReLU function should be used in the middle of two GCNConv modules.\n",
        "\n",
        "        ############# Your code here ############\n",
        "        ## (~3 line of code)\n",
        "\n",
        "        output = self.conv1(node_feature, edge_index)\n",
        "        output = self.act(output)\n",
        "        output = self.conv2(output, edge_index)\n",
        "\n",
        "        #########################################\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHgZ1ozBsYXN"
      },
      "source": [
        "## Training and Testing\n",
        "\n",
        "Now we can try to construct training and testing pipeline, which is similar to what we do in colab0. First we initialize a GCN model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "d0hojVFN1iMm"
      },
      "outputs": [],
      "source": [
        "hidden_channels = 200\n",
        "num_features = dataset.num_features\n",
        "num_classes = dataset.num_classes  # please write down the number of classes\n",
        "\n",
        "model = GCN(num_features, hidden_channels, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mA96Hyl2B_v"
      },
      "source": [
        "Then we define the optimizer and loss function. Since it is a classification task, we use Cross Entropy Loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "nPOFix9C2BRb"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCbOTE893mR6"
      },
      "source": [
        "### Question 4 (10 points)\n",
        "\n",
        "Please follow the instruction and implement a function that trains a model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "45-zRkbN3lxF"
      },
      "outputs": [],
      "source": [
        "def train(model, data, optimizer, loss_fn):\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    # TODO: Define train function.\n",
        "    # 1. put the model into train mode\n",
        "    # 2. clear the gradients calculated from the last batch\n",
        "    # 3. get the prediction by model\n",
        "    # 4. calculate the loss between our predictions and the actual labels. \n",
        "    # Just using nodes in train set!\n",
        "    # 5. calculate the gradients of each parameter\n",
        "    # 6. update the parameters by taking an optimizer step\n",
        "\n",
        "    ############# Your code here ############\n",
        "    ## (~7 line of code)\n",
        "\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    #########################################\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxgTvA6U_3-B"
      },
      "source": [
        "### Question 5 (10 points)\n",
        "\n",
        "Please follow the instruction and implement a function that evaluates a model in train, valid and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "E60aSasX_-Qe"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def test(model, data):\n",
        "\n",
        "    accuracy_list = [0, 0, 0]\n",
        "\n",
        "    # TODO: Define test function.\n",
        "    # 1. put the model into eval mode\n",
        "    # 2. get the prediction by model\n",
        "    # 3. calculate the accuracy for each set\n",
        "    # NOTE: the results should be a list containing the accuracy of different set\n",
        "\n",
        "    ############# Your code here ############\n",
        "    ## (~7 line of code)\n",
        "\n",
        "    model.eval()\n",
        "    pred = model(data.x, data.edge_index).argmax(dim=-1)\n",
        "\n",
        "    for i, mask in enumerate([data.train_mask, data.val_mask, data.test_mask]):\n",
        "        accuracy_list[i] = int((pred[mask] == data.y[mask]).sum()) / int(mask.sum())\n",
        "\n",
        "    #########################################\n",
        "\n",
        "    return accuracy_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lDrWoNtBbpi"
      },
      "source": [
        "We can start to train our model with `train` and `test` functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsLEBfWDBu2k",
        "outputId": "af3104a6-d68b-4ff1-bfc4-13ee9049326e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "after 50 epochs' training, the best test accuracy is 0.741\n"
          ]
        }
      ],
      "source": [
        "epochs = 50\n",
        "\n",
        "best_val_acc = final_test_acc = 0\n",
        "for epoch in range(1, epochs + 1):\n",
        "    loss = train(model, data, optimizer, loss_fn)\n",
        "    train_acc, val_acc, test_acc = test(model, data)\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        final_test_acc = test_acc\n",
        "print(\"after {} epochs' training, the best test accuracy is {}\".format(epochs, final_test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm2y-VKOvOop"
      },
      "source": [
        "## Submission\n",
        "\n",
        "Make sure to run all the cells and save a copy of this colab in your driver. If you complete this notebook, download the colab and upload your work to canvas to submit it."
      ]
>>>>>>> b57546b7fc54e186149c662e4e05b2b8d64a22bf
    }
   ],
   "source": [
    "# import the pytorch library into environment and check its version\n",
    "import os\n",
    "import torch\n",
    "print(\"Using torch\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MC5DJkYNWGeL"
   },
   "source": [
    "Let's start installing PyG by `pip`. The version of PyG should match the current version of PyTorch. Here we follow the [instruction](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html) of PyG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NWtNbC9FgZOM",
    "outputId": "6345ee90-007a-4326-c028-972d0c118824"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Looking in links: https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
      "Collecting torch-scatter\n",
      "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.9 MB 12.6 MB/s \n",
      "\u001b[?25hCollecting torch-sparse\n",
      "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_sparse-0.6.15-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.5 MB 58.9 MB/s \n",
      "\u001b[?25hCollecting torch-cluster\n",
      "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl (2.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4 MB 21.9 MB/s \n",
      "\u001b[?25hCollecting torch-spline-conv\n",
      "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (709 kB)\n",
      "\u001b[K     |████████████████████████████████| 709 kB 50.8 MB/s \n",
      "\u001b[?25hCollecting torch-geometric\n",
      "  Downloading torch_geometric-2.1.0.post1.tar.gz (467 kB)\n",
      "\u001b[K     |████████████████████████████████| 467 kB 15.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.7.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.64.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.9)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2022.6.15)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
      "Building wheels for collected packages: torch-geometric\n",
      "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for torch-geometric: filename=torch_geometric-2.1.0.post1-py3-none-any.whl size=689859 sha256=cfe188a0abaf616cabdce85b38c3e5937ae23d1bb42be26fcd5674d49fcb81c4\n",
      "  Stored in directory: /root/.cache/pip/wheels/d1/cb/43/f7f2e472de4d7cff31bceddadc36d634e1e545fbc17961c282\n",
      "Successfully built torch-geometric\n",
      "Installing collected packages: torch-spline-conv, torch-sparse, torch-scatter, torch-geometric, torch-cluster\n",
      "Successfully installed torch-cluster-1.6.0 torch-geometric-2.1.0.post1 torch-scatter-2.0.9 torch-sparse-0.6.15 torch-spline-conv-1.2.1\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting ogb\n",
      "  Downloading ogb-1.3.4-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 230 kB/s \n",
      "\u001b[?25hRequirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.24.3)\n",
      "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (4.64.0)\n",
      "Collecting outdated>=0.2.0\n",
      "  Downloading outdated-0.2.1-py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.0.2)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.3.5)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.12.1+cu113)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.21.6)\n",
      "Collecting littleutils\n",
      "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (2.23.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2022.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->ogb) (4.1.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2022.6.15)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n",
      "Building wheels for collected packages: littleutils\n",
      "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=c559dde34c613b4eaa975981a86e1b813719f2c4ace3512fcce1cfb7c822862e\n",
      "  Stored in directory: /root/.cache/pip/wheels/d6/64/cd/32819b511a488e4993f2fab909a95330289c3f4e0f6ef4676d\n",
      "Successfully built littleutils\n",
      "Installing collected packages: littleutils, outdated, ogb\n",
      "Successfully installed littleutils-0.2.2 ogb-1.3.4 outdated-0.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
    "!pip install ogb  # for datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ei5_O8ZXtlGb"
   },
   "source": [
    "### Create a Graph\n",
    "\n",
    "A single graph in PyG is described by an instance of `torch_geometric.data` which holds the some important attributes by default, like edge_index. We can easily create a graph of various number of edges and nodes by PyG. Take the following graph as an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNeApmAljKyq"
   },
   "source": [
    "![](https://github.com/Graph-and-Geometric-Learning/CPSC483-colab/blob/main/fig/graph_example.png?raw=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZSndMSIJhvC6"
   },
   "outputs": [],
   "source": [
    "# import torch_geometric.data into environment\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6Usm6XTk22d"
   },
   "source": [
    "We have 6 edges (undirected graph) and 3 nodes in this graph. So the edge index can be defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fAqsazT4lPAZ"
   },
   "outputs": [],
   "source": [
    "edge_index = torch.tensor([[0, 1, 1, 2, 0, 2],\n",
    "                           [1, 0, 2, 1, 2, 0]], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each edge is represented as a tuple (u, v), and that edge_index consists of num_edges columns where each column consists of the two indices u and v corresponding to each edge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fw5y3_Ulx1T"
   },
   "source": [
    "Besides, each node can have a node feature which describes the node's property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3doHCX65l63N"
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([[-1], [0], [1]], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGKFW9sRlnFK"
   },
   "source": [
    "Then we can define a `Data` object with edge index and node attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6mmud5UqlfM9"
   },
   "outputs": [],
   "source": [
    "data = Data(x=x, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Af1qVLlmCNb"
   },
   "source": [
    "`Data` object supports many useful utility functions. For example, we can see the number of the nodes, and whether the graph is a undirected graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ybw_wwuStyHo",
    "outputId": "ba68da22-599c-43f0-cdcf-793cd184b508"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nodes is: 3\n",
      "graph is directed or not: False\n"
     ]
    }
   ],
   "source": [
    "num_nodes = data.num_nodes\n",
    "print(\"number of nodes is:\", num_nodes)\n",
    "\n",
    "is_directed = data.is_directed()\n",
    "print(\"graph is directed or not:\", is_directed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCY73L2LGoS8"
   },
   "source": [
    "### Question 1 (5 points)\n",
    "\n",
    "What is the number of the neighbors of node 0 in the graph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HMg1NUP9Gnkq",
    "outputId": "7a1d32d1-8087-4922-e16c-b167d79c0959"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node with index 0 has 2 neighbors\n"
     ]
    }
   ],
   "source": [
    "def get_n_neighbors(graph, idx):\n",
    "  # TODO: Implement a function that takes a Data object,\n",
    "  # an index of a node, and returns the number of the neighbors \n",
    "  # of this node (as an integer).\n",
    "\n",
    "  n_neighbors = 0\n",
    "\n",
    "  ############# Your code here ############\n",
    "  ## (~1 line of code)\n",
    "\n",
    "  n_neighbors = (data.edge_index[0] == idx).sum().item()\n",
    "\n",
    "  #########################################\n",
    "\n",
    "  return n_neighbors\n",
    "\n",
    "idx = 0\n",
    "n_neighbors = get_n_neighbors(data, idx)\n",
    "print('Node with index {} has {} neighbors'.format(idx, n_neighbors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKHOYbMCE9b7"
   },
   "source": [
    "PyG has a number of graph data with various scales. Cora is one of the most famous dataset in graph learning, and we can use it by PyG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RfZq2L2uEphL",
    "outputId": "845e9e74-ab12-43ea-d180-4b6948720fab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "dataset = Planetoid('/tmp/cora', 'cora')\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2h5lpJYpjV0"
   },
   "source": [
    "We can see the number of the nodes and edges in cora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UnYTDzQFpi40",
    "outputId": "c7ebbf53-6146-4703-a152-111d126ae9bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cora has 2708 nodes\n",
      "cora has 10556 edges\n"
     ]
    }
   ],
   "source": [
    "num_nodes = data.num_nodes\n",
    "print('cora has {} nodes'.format(num_nodes))\n",
    "\n",
    "num_edges = data.num_edges\n",
    "print('cora has {} edges'.format(num_edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HebClViZqU1V"
   },
   "source": [
    "### Question 2 (10 points)\n",
    "\n",
    "1. What is the number of the classes in cora dataset? \n",
    "2. Which node in Cora has the most number of neighbors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pVuXmtFnqr4e",
    "outputId": "49e214e8-c9f0-49ad-98d4-46bbead6d43f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cora has 7 classes\n",
      "1358 in cora has the most number of neighbors\n"
     ]
    }
   ],
   "source": [
    "def get_num_classes(data):\n",
    "  # TODO: Implement a function that takes a dataset object\n",
    "  # and returns the number of classes for that dataset.\n",
    "\n",
    "  num_classes = 0\n",
    "\n",
    "  ############# Your code here ############\n",
    "  ## (~1 line of code)\n",
    "  \n",
    "  num_classes = data.y.unique().shape[0]\n",
    "\n",
    "  #########################################\n",
    "\n",
    "  return num_classes\n",
    "\n",
    "def get_idx_with_most_neighbors(data):\n",
    "  # TODO: Implement a function that takes a dataset object\n",
    "  # and returns the index of the node which has the most number of neighbors.\n",
    "\n",
    "  idx = -1\n",
    "\n",
    "  ############# Your code here ############\n",
    "  ## (~3 line of code)\n",
    "\n",
    "  n_nodes = data.x.shape[0]\n",
    "  degrees = [get_n_neighbors(data, i) for i in range(n_nodes)]\n",
    "  idx = degrees.index(max(degrees))\n",
    "  \n",
    "  #########################################\n",
    "\n",
    "  return idx\n",
    "\n",
    "num_classes = get_num_classes(data)\n",
    "print(\"cora has {} classes\".format(num_classes))\n",
    "\n",
    "idx = get_idx_with_most_neighbors(data)\n",
    "print(\"{} in cora has the most number of neighbors\".format(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiQW1Hs46Kpv"
   },
   "source": [
    "In cora, we split the data into train set, validation set and test set by node mask. All the nodes will participate in the message passing process, but we can only assess the train label during training process. This is what we call [transductive learning](https://en.wikipedia.org/wiki/Transduction_(machine_learning))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ioXYJFV-7UU_",
    "outputId": "cd0a9628-95e0-4ba5-91f2-da873ec7c9aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nodes in train set, 140\n",
      "number of nodes in valid set, 500\n",
      "number of nodes in test set, 1000\n"
     ]
    }
   ],
   "source": [
    "node_feature = data.x\n",
    "\n",
    "train_node_feature = node_feature[data.train_mask]\n",
    "valid_node_feature = node_feature[data.val_mask]\n",
    "test_node_feature = node_feature[data.test_mask]\n",
    "\n",
    "print(\"number of nodes in train set,\", train_node_feature.shape[0])\n",
    "print(\"number of nodes in valid set,\", valid_node_feature.shape[0])\n",
    "print(\"number of nodes in test set,\", test_node_feature.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhLEhOimr7mY"
   },
   "source": [
    "## Build a GNN by PyG\n",
    "\n",
    "In this section we will use PyG to build a classic graph neural network called GCN([Kipf et al. (2017)](https://arxiv.org/pdf/1609.02907.pdf)). Then we will apply this model to handle node classification task in cora.\n",
    "A GCN is built by stacking multiple graph convolution layers `GCNConv` which passes the messages from neighbors to the center node. Here we can define a `GCNConv` by PyG:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "x-8eQfROu7Hm"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "conv = GCNConv(in_channels=1433, out_channels=200, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxNlpZ5cvI9s"
   },
   "source": [
    "`in_channels` is the dimension of node's input feature, `out_channels` is the  dimension of the output representation of node, and `normalize` is whether to add self-loops and compute symmetric normalization on the adjacent matrix. \n",
    "The feature's dimension in cora is 1433, so `in_channels` is set as 1433. We can perform a message passing on cora like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MlwiHnm7wy8S",
    "outputId": "5ddb3c58-b508-4611-c5d3-972f2746bcfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension of node_feature: torch.Size([2708, 1433])\n",
      "dimension of node_representation: torch.Size([2708, 200])\n"
     ]
    }
   ],
   "source": [
    "node_feature = data.x\n",
    "edge_index = data.edge_index\n",
    "\n",
    "node_representation = conv(node_feature, edge_index)\n",
    "\n",
    "print(\"dimension of node_feature:\", node_feature.shape)\n",
    "print(\"dimension of node_representation:\", node_representation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxItCC52xud5"
   },
   "source": [
    "We can see that the inputs of `GCNConv` are node feature and edge index. Then the convolution module will perform a message passing like GCN. \n",
    "Recall the MLP we build in colab0. Here we also use `nn.Module` to define a MLP class containing the basic modules of GCN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eug5Vv-Py5f_"
   },
   "source": [
    "### Question 3 (10 points)\n",
    "\n",
    "Following the instruction and build a GCN class using the `GCNConv` modules. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "iDO3ucqjtBF6"
   },
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Define two GCNConv modules and a ReLU function.\n",
    "        # The input size and output size of first GCNConv module should be in_channels and hidden_channels\n",
    "        # The input size and output size of second GCNConv module should be hidden_channels and out_channels\n",
    "\n",
    "        ############# Your code here ############\n",
    "        ## (~3 line of code)\n",
    "\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "        self.act = torch.nn.ReLU()\n",
    "\n",
    "        #########################################\n",
    "\n",
    "    def forward(self, node_feature, edge_index):\n",
    "\n",
    "        output = None\n",
    "\n",
    "        # TODO: Use the modules you define in __init__ to perform message passing.\n",
    "        # ReLU function should be used in the middle of two GCNConv modules.\n",
    "\n",
    "        ############# Your code here ############\n",
    "        ## (~3 line of code)\n",
    "\n",
    "        output = self.conv1(node_feature, edge_index)\n",
    "        output = self.act(output)\n",
    "        output = self.conv2(output, edge_index)\n",
    "\n",
    "        #########################################\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHgZ1ozBsYXN"
   },
   "source": [
    "## Training and Testing\n",
    "\n",
    "Now we can try to construct training and testing pipeline, which is similar to what we do in colab0. First we initialize a GCN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "d0hojVFN1iMm"
   },
   "outputs": [],
   "source": [
    "hidden_channels = 200\n",
    "num_features = dataset.num_features\n",
    "num_classes = dataset.num_classes  # please write down the number of classes\n",
    "\n",
    "model = GCN(num_features, hidden_channels, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mA96Hyl2B_v"
   },
   "source": [
    "Then we define the optimizer and loss function. Since it is a classification task, we use Cross Entropy Loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "nPOFix9C2BRb"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCbOTE893mR6"
   },
   "source": [
    "### Question 4 (10 points)\n",
    "\n",
    "Please follow the instruction and implement a function that trains a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "45-zRkbN3lxF"
   },
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, loss_fn):\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # TODO: Define train function.\n",
    "    # 1. put the model into train mode\n",
    "    # 2. clear the gradients calculated from the last batch\n",
    "    # 3. get the prediction by model\n",
    "    # 4. calculate the loss between our predictions and the actual labels. \n",
    "    # Just using nodes in train set!\n",
    "    # 5. calculate the gradients of each parameter\n",
    "    # 6. update the parameters by taking an optimizer step\n",
    "\n",
    "    ############# Your code here ############\n",
    "    ## (~7 line of code)\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #########################################\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxgTvA6U_3-B"
   },
   "source": [
    "### Question 5 (10 points)\n",
    "\n",
    "Please follow the instruction and implement a function that evaluates a model in train, valid and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "E60aSasX_-Qe"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, data):\n",
    "\n",
    "    accuracy_list = [0, 0, 0]\n",
    "\n",
    "    # TODO: Define test function.\n",
    "    # 1. put the model into eval mode\n",
    "    # 2. get the prediction by model\n",
    "    # 3. calculate the accuracy for each set\n",
    "    # NOTE: the results should be a list containing the accuracy of different set\n",
    "\n",
    "    ############# Your code here ############\n",
    "    ## (~7 line of code)\n",
    "\n",
    "    model.eval()\n",
    "    pred = model(data.x, data.edge_index).argmax(dim=-1)\n",
    "\n",
    "    for i, mask in enumerate([data.train_mask, data.val_mask, data.test_mask]):\n",
    "        accuracy_list[i] = int((pred[mask] == data.y[mask]).sum()) / int(mask.sum())\n",
    "\n",
    "    #########################################\n",
    "\n",
    "    return accuracy_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lDrWoNtBbpi"
   },
   "source": [
    "We can start to train our model with `train` and `test` functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GsLEBfWDBu2k",
    "outputId": "af3104a6-d68b-4ff1-bfc4-13ee9049326e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 50 epochs' training, the best test accuracy is 0.741\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "best_val_acc = final_test_acc = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss = train(model, data, optimizer, loss_fn)\n",
    "    train_acc, val_acc, test_acc = test(model, data)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        final_test_acc = test_acc\n",
    "print(\"after {} epochs' training, the best test accuracy is {}\".format(epochs, final_test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dm2y-VKOvOop"
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure to run all the cells and save a copy of this colab in your driver. If you complete this notebook, download the colab and upload your work to canvas to submit it."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ddiMjUdbmW24",
    "H1ikBfJFZ1iy",
    "IEvPG5hIfuU5",
    "1SZax14Sg61s",
    "lqkSkLWKC7ku",
    "Ope6QD8w8w3S"
   ],
   "include_colab_link": true,
   "name": "CPSC483_colab1",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
